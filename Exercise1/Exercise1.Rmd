---
subtitle: "TMA4268 Statistical Learning V2023"
title: "Compulsory exercise 1: Group 3"
author: "Helle Villmones Haug, Hjalmar Jacob Vinje and Sanna Baug Warholm"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```

```{r,eval=FALSE,echo=FALSE}
install.packages("knitr")     # probably already installed
install.packages("rmarkdown") # probably already installed
install.packages("ggplot2")   # plotting with ggplot2
install.packages("dplyr")     # for data cleaning and preparation
install.packages("tidyr")     # also data preparation
install.packages("carData")   # dataset
install.packages("class")     # for KNN
install.packages("pROC")      # calculate roc
install.packages("plotROC")   # plot roc
install.packages("ggmosaic")  # mosaic plot
```

```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
```

<!--  Etc (load all packages needed). -->


# Problem 1

For this problem you will need to include some LaTex code. Please install latex on your computer and then consult Compulsor1.Rmd for hints how to write formulas in LaTex.

## a)

We know
$$
Y=f(\mathbf {x})+\varepsilon \\
\text{E}(\varepsilon)=0 \\
f(\mathbf{x}) = \mathbf{x}^T\boldsymbol{\beta}
$$

so

$$\text{E}(\widetilde{\boldsymbol \beta}) =(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\text{E}({\bf Y}) = \\
(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\text{E}(\mathbf{X}\beta+\epsilon) = \\
(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\mathbf{X}\beta+0 = \\
(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\mathbf{X}\beta $$

And the covariance matrix is then 

$$\text{Cov}(\widetilde{\boldsymbol \beta}) = \text{Cov}((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top{\bf Y}) = \\
(\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top \text{Cov}(\bf{Y}) ((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top
$$
Cov(Y) would be only the variance: 

$$
 = (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top \sigma^{2} \mathbf{I}  ((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top
$$


## b)

Expected value: 
$$ \text{E}(\widetilde{f}({x_0})) = \text{E}(\mathbf{x}^\top_0 \widetilde{\boldsymbol \beta}) = \mathbf{x}^\top_0 \text{E}(\widetilde{\boldsymbol \beta}) = \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\mathbf{X}\beta $$
Variance: 

$$ \text{Var}(\widetilde{f}({x_0})) = \text{Var}(\mathbf{x}^\top_0 \widetilde{\boldsymbol \beta}) = \mathbf{x}^\top_0 \text{Var}(\widetilde{\boldsymbol \beta}) \mathbf{x}_0 = \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top \text{Cov}(\bf{Y}) ((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0 $$
Cov(Y) would be only the variance: 

$$ = \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top \sigma^{2} \mathbf{I} ((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0 $$


## c)

Bias is a systematic error, in statistics it is an estimate of the systematic difference from the true value. If a model is inflexible, it could be underfitted and have a high bias. 

Variance measures uncertainty. If variance is high, the estimate of the distribution function is likely to change for different input data. Very complex models might overfit data and get a high variance. 

Irreducable error is the variance of a "unknown" variable which adds uncorrelated noise with a mean of 0 ("random", or unobserved influences), which means that it cannot be reduced by improving the fit of the model. 


## d)

Bias: 

$$ f({x_0}) - E[\widetilde{f}({x_0})] = \mathbf{x}^\top_0 \widetilde{\boldsymbol \beta} - \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\mathbf{X}\beta  $$

Variance (using previous calculations): 

$$  Var(\widetilde{f}({x_0})) = \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top \text{Cov}(\bf{Y}) ((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0 $$


Irreducible error:

$$ Var(\varepsilon)= \sigma^{2} $$

MSE:

$$ MSE = \text{irreducable error + variance + squared bias} = \\ 
Var(\varepsilon) + \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top \text{Cov}(\bf{Y}) ((\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top)^\top \mathbf{x}_0 + (\mathbf{x}^\top_0 \widetilde{\boldsymbol \beta} - \mathbf{x}^\top_0 (\mathbf{X}^\top\mathbf{X}+\lambda {\bf I})^{-1}\mathbf{X}^\top\mathbf{X}\beta)^2 $$


## e)

 
```{r}
id <- "1X_8OKcoYbng1XvYFDirxjEWr7LtpNr1m" # google file ID
values <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
X <- values$X
dim(X)
x0 <- values$x0
dim(x0)
beta <- values$beta
dim(beta)
sigma <- values$sigma
sigma
```

```{r,eval=FALSE}
library(ggplot2)
bias <- function(lambda, X, x0, beta) {
  p <- ncol(X)
  value <- (t(x0) %*% beta - t(x0) %*% solve(t(X) %*% X + lambda * diag(p)) %*% t(X) %*% X %*% beta)^2
  return(value)
}
lambdas <- seq(0, 2, length.out = 500)
BIAS <- rep(NA, length(lambdas))
for (i in seq_along(lambdas)) BIAS[i] <- bias(lambdas[i], X, x0, beta)
dfBias <- data.frame(lambdas = lambdas, bias = BIAS)
ggplot(dfBias, aes(x = lambdas, y = bias)) +
  geom_line(color = "hotpink") +
  xlab(expression(lambda)) +
  ylab(expression(bias^2))
```



## f)

Now we will create the variance function which takes the same inputs as the squared bias. As in e) you have to fill only the `value <- ... ` and run the code to plot the variance.

```{r,eval=FALSE}
variance <- function(lambda, X, x0, sigma) {
  p <- ncol(X)
  inv <- solve(t(X) %*% X + lambda * diag(p))
  value <- t(x0) %*% solve(t(X) %*% X + lambda * diag(p)) %*% t(X) %*% t(solve(t(X) %*% X + lambda * diag(p)) %*% t(X)) %*% x0 * sigma^2
  return(value)
}
lambdas <- seq(0, 2, length.out = 500)
VAR <- rep(NA, length(lambdas))
for (i in seq_along(lambdas)) VAR[i] <- variance(lambdas[i], X, x0, sigma)
dfVar <- data.frame(lambdas = lambdas, var = VAR)
ggplot(dfVar, aes(x = lambdas, y = var)) +
  geom_line(color = "gold") +
  xlab(expression(lambda)) +
  ylab("variance")
```


## g)

Fill in the `exp_mse` of the following code to calculate the expected MSE for the same lambda values that you plugged in above. Then plot all the components together and find the value of $\lambda$ which minimizes the expected MSE. 

```{r,eval=FALSE}
irrErr <- sigma^2
exp_mse <- BIAS + VAR + irrErr
min_lambda <- lambdas[which.min(exp_mse)]
dfMSE <- data.frame(lambdas = lambdas, mse = exp_mse, bias = BIAS, var = VAR, irrErr = irrErr)
dfBVI <- merge(dfBias, dfVar, by = "lambdas")

ggplot(dfMSE, aes(x = lambdas)) + 
  geom_line(aes(y = bias, color = "BIAS squared")) + 
  geom_line(aes(y = var, color = "VAR")) + 
  geom_line(aes(y = irrErr, color = "irrErr")) + 
  geom_line(aes(y = mse, color = "Exp. MSE")) +
  scale_color_manual(values = c("BIAS" = "blue", "VAR" = "orange", "irrErr" = "pink", "Exp. MSE" = "purple")) +
  xlab("lambda") +
  ylab("Exp. MSE") +
  ggtitle(paste("the lambda that minimize expected MSE is ", round(min_lambda, 3)))

```



# Problem 2

## a)

### i) 
The lm() function creates a variable to be estimatied which is multiplied with the binary variable of rankAssocProf and rankProf. The interpretation oof the number corresponding to the variables is that holding all other variables constant going from a AsstProf to AssocProf is assosiated with an increase in salary of 12,907.6 and going from AsstProf to Prof is assosiated with an increase in salary of 45,066.0 holding all other variables constant.

### ii)
To test the whole categorical variable Rank, you can use an ANOVA test to compare the means of salary for each category of Rank. 

````
             Df    Sum Sq   Mean Sq F value Pr(>F)    
rank          2 1.432e+11 7.162e+10   128.2 <2e-16 ***
Residuals   394 2.201e+11 5.586e+08                   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
````

This shows that rank as a whole has an impact on salary at the 99% confidence level.

## b) 


 ```{r, echo=FALSE, results='asis'}
# Plot 1 - Salary by rank
library(ggplot2)
# calculate the average salary for each rank
avg_salary_by_rank <- aggregate(Salaries$salary, by = list(Salaries$rank), mean)
names(avg_salary_by_rank) <- c("rank", "avg_salary")

# add the geom_text layer to display the calculated average salaries
ggplot(Salaries, aes(x = rank, y = salary)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_hline(data = avg_salary_by_rank, aes(yintercept = avg_salary), color = "red", linetype = "dotted") +
  geom_text(data = avg_salary_by_rank, aes(x = rank, y = avg_salary, label = paste("Avg:", round(avg_salary, 0))), hjust = -0.05, vjust = -0.2) +
  labs(x = "Years of Service", y = "Salary")
 

 library(dplyr)
# Calculate the proportions of each rank made up by women
Salaries_proportions <- Salaries %>%
  group_by(rank, sex) %>%
  summarise(n = n()) %>%
  mutate(proportion = n / sum(n))

# Create a bar chart of the number of males and females for each rank
ggplot(Salaries_proportions, aes(x=rank, y=proportion, fill=sex)) + 
  geom_bar(stat="identity", position="dodge") + 
  labs(x="Rank", y="Proportion", fill="Sex") + 
  scale_y_continuous(labels = scales::percent) + 
  geom_text(aes(label = scales::percent(proportion), y = proportion), 
            position = position_dodge(0.9), vjust = -0.5) +
  theme_classic()
 
 ```
 
The first graph shows that professors earn more than assistante professors and associate professors. The second shows that women make up a higher proportion of AsstProf than Prof.

This is the reason why the regression with sex as the only covariate shows that sex is statistically significant in trems of salary, but it is no longer significant when controlling for rank and the other covariates.

## c)

```{r fig_model, fig.width=7, fig.height=7, fig.cap="Diagnostic for model1",out.width = '70%'}
model1 <- lm(salary ~ ., data = Salaries)
options(repos = c(CRAN = "https://cran.rstudio.com/"))
install.packages("ggfortify")
library(ggplot2)
library(ggfortify)
autoplot(model1, smooth.colour = NA)
```
The first plot showing residuals vs fitted values shows that the data is heteroskedastic, meaning that the variance is not constant. This breaks the assumption of homoscedasticity. 

# ii)
```{r fig_model_check, fig.width=7, fig.height=7, fig.cap="Diagnostic for model2",out.width = '70%'}
log_salary = log(Salaries$salary)
model2 <- lm(log_salary ~ rank + discipline + yrs.since.phd + yrs.service + sex - salary, data = Salaries)
library(ggplot2)
autoplot(model2, smooth.colour = NA)
```

The residual variance is still heteroskedastic. 

```{r fig_model_check_log-transform, fig.width=7, fig.height=7, fig.cap="Diagnostic for model3",out.width = '70%'}
log_salary = log(Salaries$salary)
model2 <- lm(log_salary ~ rank + discipline + yrs.since.phd + yrs.service + sex - salary, data = Salaries)
library(ggplot2)
autoplot(model2, smooth.colour = NA)
```
## d)

)
```{r fig_model_check_interaction, fig.width=6, fig.height=6, fig.cap="Diagnostic plots for `model3`.",out.width = '70%'}
model3 <- lm(log_salary ~ rank + discipline + yrs.since.phd + yrs.service + sex + sex:yrs.since.phd -salary, data = Salaries)
summary(model3)
```

ii) The interaction term is not statistically significant, so we can not conclude that Bert-Ernie is correct.

## e)

```{r bootstrap}
# i)
set.seed(4268)
getR2 <- function(data, indices) 
  {fit <- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex,
  data = data[indices,])
summary(fit)$r.squared}
library(boot)
boot_results <- boot(data = Salaries, statistic = getR2, R = 1000, strata = Salaries$rank)
```

```{r bootstrap_plot, fig.width=5, fig.height=5}
# ii)
hist(boot_results$t, main = "Bootstrap distribution of R-Squared", 
     xlab = "R-Squared", col = "blue", border = "black", breaks = 30)
```

```{r bootstrap_quantiles}
# iii)
sd(boot_results$t)
quant = boot_results$t[25:975]
summary(quant)
```

(iv) R-squared was calculated to 0.5249 and the bootstrap estimates 0.4546766 with a 95% confidence interval of [0.3761, 0.5470].The original R-squared lies within the 95% confidence interval.


## f)


```{r prediction}
# i)
bert_ernie <- data.frame(rank=c("Prof", "Prof"), discipline=c("A", "B"), 
                         yrs.since.phd=c(20, 20), yrs.service=c(20, 20), 
                         sex=c("Male", "Male"))
preds <- predict(object=model1, newdata=bert_ernie, interval="prediction", level=0.95) 
# 1. Corrected confidence to prediction
# 2. Corrected 0.975 to 0.95
preds[1, 2] > 75000
```

He can now no longer be confident with 95% certainty that we will earn at least $75 000 at this time.

ii) The analytic expression for the lower limit of the prediction interval:
$$
PI_{lower} = \boldsymbol{x}_0^T \hat{\boldsymbol{\beta}} - t_{n-p} (1-\alpha/2) \hat{\sigma} \sqrt{1+ \boldsymbol{x}_0^T (X^TX)^{-1} \boldsymbol{x}_0}
$$

```{r PI_lower_calculated}
x_0 = c(1, 0, 1, 0, 20, 20, 1)
beta_hat = coef(model1)
alpha = 0.05
sd_hat = summary(model1)$sigma
X <- model.matrix(~rank+discipline+yrs.since.phd+yrs.service+sex, data=Salaries)
n = nrow(Salaries)
p = ncol(X)
PI_lower = t(x_0)%*%beta_hat - qt(1-alpha/2,df=n-p) * sd_hat * 
  sqrt(1+t(x_0)%*%solve(t(X)%*%X)%*%x_0)
PI_lower
PI_lower == preds[1,2]
```

# Problem 3

# Problem 4